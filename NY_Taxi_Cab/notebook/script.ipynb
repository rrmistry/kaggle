{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b636e22977c7364a057d29195216501485c041a7"
   },
   "source": [
    "Rohit's First Kernal - NYC Taxi Fare Prediction\n",
    "===========\n",
    "This is the first kernal for submission for Google Cloud Playground [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction)\n",
    "\n",
    "Strategy\n",
    "--------------------\n",
    "1. Filter out outliers\n",
    "    1. Remove data outside NYC\n",
    "    2. Remove data where fare is unresonable (too high / too low)\n",
    "2. Use Linear Regression ML Model On Clean Data\n",
    "3. Use Linear Fit On Unclean Data\n",
    "\n",
    "Using NYC Open Data\n",
    "-------------------\n",
    "NYC Open Data is stored in Google Big Query open datasets. To access this data in your notebook, check out kernal [How to Query the NYC Open Data\n",
    "](https://www.kaggle.com/paultimothymooney/how-to-query-the-nyc-open-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to plot 3d scatter plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import math\n",
    "\n",
    "# to print out current time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import traceback\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read exploratory dataset into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "e78d5459cdfa29acfe87fc6282a41cc90bae2128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset -------------  2018-09-20 07:20:58.171886\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'M:/kaggle/NY Taxi Cab/notebook/input/train_split\\\\train-000000000003.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-056d75597442>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Try to load the data. This may be an intensive process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASE_DATA_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'train_split\\train-000000000003.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pickup_datetime\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Finished reading dataset ------------- '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'M:/kaggle/NY Taxi Cab/notebook/input/train_split\\\\train-000000000003.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# BASE_DATA_PATH = os.path.dirname(\"__file__\") + r'..\\input\\'\n",
    "BASE_DATA_PATH = r'M:/kaggle/NY Taxi Cab/input/'\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "print('Started reading dataset ------------- ', datetime.datetime.now())\n",
    "\n",
    "# Try to load the data. This may be an intensive process\n",
    "df_train = pd.read_csv(os.path.join(BASE_DATA_PATH, r'train_split\\train-000000000003.csv'), nrows=BATCH_SIZE*2, parse_dates=[\"pickup_datetime\"]);\n",
    "\n",
    "print('Finished reading dataset ------------- ', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe some dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training dataset properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = 'key,key_original,fare_amount,pickup_datetime,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers'.split(',')\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "DEFAULTS = [['nokey'], ['nokey'], [0.0], ['badDate'], ['Sun'], [0], [-74.0], [40.0], [-74.0], [40.7], [0.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the raw input columns, and will be provided for prediction also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMNS = [\n",
    "    # Define features\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('dayofweek', vocabulary_list = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']),\n",
    "    tf.feature_column.categorical_column_with_identity('hourofday', num_buckets = 24),\n",
    "\n",
    "    # Numeric columns\n",
    "    tf.feature_column.numeric_column('pickuplat'),\n",
    "    tf.feature_column.numeric_column('pickuplon'),\n",
    "    tf.feature_column.numeric_column('dropofflat'),\n",
    "    tf.feature_column.numeric_column('dropofflon'),\n",
    "    tf.feature_column.numeric_column('passengers'),\n",
    "    \n",
    "    # Engineered features that are created in the input_fn\n",
    "    tf.feature_column.numeric_column('latdiff'),\n",
    "    tf.feature_column.numeric_column('londiff'),\n",
    "    tf.feature_column.numeric_column('euclidean')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eval_metrics(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    return {\n",
    "        'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, nbuckets, hidden_units):\n",
    "    \"\"\"\n",
    "     Build an estimator starting from INPUT COLUMNS.\n",
    "     These include feature transformations and synthetic features.\n",
    "     The model is a wide-and-deep model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input columns\n",
    "    (dayofweek, hourofday, plat, plon, dlat, dlon, pcount, latdiff, londiff, euclidean) = INPUT_COLUMNS\n",
    "\n",
    "    # Bucketize the lats & lons\n",
    "    latbuckets = np.linspace(37.0, 45.0, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(-78.0, -70.0, nbuckets).tolist()\n",
    "    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n",
    "    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n",
    "    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n",
    "    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n",
    "\n",
    "    # Feature cross\n",
    "    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n",
    "    day_hr =  tf.feature_column.crossed_column([dayofweek, hourofday], 24 * 7)\n",
    "\n",
    "    # Wide columns and deep columns.\n",
    "    wide_columns = [\n",
    "        # Feature crosses\n",
    "        dloc, ploc, pd_pair,\n",
    "        day_hr,\n",
    "\n",
    "        # Sparse columns\n",
    "        dayofweek, hourofday,\n",
    "\n",
    "        # Anything with a linear relationship\n",
    "        pcount \n",
    "    ]\n",
    "\n",
    "    deep_columns = [\n",
    "        # Embedding_column to \"group\" together ...\n",
    "        tf.feature_column.embedding_column(pd_pair, int(nbuckets/4)),\n",
    "        tf.feature_column.embedding_column(day_hr, int(nbuckets/4)),\n",
    "\n",
    "        # Numeric columns\n",
    "        plat, plon, dlat, dlon,\n",
    "        latdiff, londiff, euclidean\n",
    "    ]\n",
    "    \n",
    "    ## setting the checkpoint interval to be much lower for this task\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = 30, \n",
    "                                        keep_checkpoint_max = 10)\n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = model_dir,\n",
    "        dnn_activation_fn=tf.nn.relu,\n",
    "        dnn_optimizer='Adagrad',\n",
    "        linear_feature_columns = wide_columns,\n",
    "        dnn_feature_columns = deep_columns,\n",
    "        dnn_hidden_units = hidden_units,\n",
    "        config = run_config)\n",
    "\n",
    "    # add extra evaluation metric for hyperparameter tuning\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, add_eval_metrics)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature engineering function that will be used in the input and serving input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered(features):\n",
    "    # this is how you can do feature engineering in TensorFlow\n",
    "    lat1 = features['pickuplat']\n",
    "    lat2 = features['dropofflat']\n",
    "    lon1 = features['pickuplon']\n",
    "    lon2 = features['dropofflon']\n",
    "    latdiff = (lat1 - lat2)\n",
    "    londiff = (lon1 - lon2)\n",
    "    \n",
    "    # set features for distance with sign that indicates direction\n",
    "    features['latdiff'] = latdiff\n",
    "    features['londiff'] = londiff\n",
    "    dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "    features['euclidean'] = dist\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create serving input function to be able to serve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        # All the real-valued columns\n",
    "        column.name: tf.placeholder(tf.float32, [None]) for column in INPUT_COLUMNS[2:7]\n",
    "    }\n",
    "    feature_placeholders['dayofweek'] = tf.placeholder(tf.string, [None])\n",
    "    feature_placeholders['hourofday'] = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    features = add_engineered(feature_placeholders.copy())\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input function to load data into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "        \n",
    "        def parse_dataset(filename, header_lines = 1):\n",
    "            return tf.data.TextLineDataset(filenames=filename).skip(header_lines)\n",
    "        \n",
    "        def parse_batch(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return add_engineered(features), label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        filenames_dataset = tf.data.Dataset.list_files(filename)\n",
    "        \n",
    "        # Read lines from text files\n",
    "        dataset = filenames_dataset.flat_map(parse_dataset)\n",
    "        \n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(parse_batch)\n",
    "        \n",
    "        # Note:\n",
    "        # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename  -> text lines)\n",
    "        # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 10 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        batch_features, batch_labels = dataset.make_one_shot_iterator().get_next()\n",
    "        return batch_features, batch_labels\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create estimator train and evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(args):\n",
    "    \n",
    "    estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args['train_data_paths'],\n",
    "            mode = tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size = args['train_batch_size']),\n",
    "        max_steps = args['train_steps'])\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args['eval_data_paths'],\n",
    "            mode = tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size = args['eval_batch_size']),\n",
    "        steps = 100,\n",
    "        exporters = exporter)\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_evaluation_master': '', '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E0FE3C79B0>, '_save_checkpoints_steps': None, '_train_distribute': None, '_session_config': None, '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_service': None, '_keep_checkpoint_max': 10, '_model_dir': 'C:\\\\Users\\\\mistr\\\\source\\\\repos\\\\rrmistry\\\\kaggle\\\\NY_Taxi_Cab\\\\ML_Model', '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_log_step_count_steps': 100, '_task_type': 'worker', '_task_id': 0, '_device_fn': None, '_save_checkpoints_secs': 30, '_master': '', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Using config: {'_evaluation_master': '', '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E0FE3C7B00>, '_save_checkpoints_steps': None, '_train_distribute': None, '_session_config': None, '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_service': None, '_keep_checkpoint_max': 10, '_model_dir': 'C:\\\\Users\\\\mistr\\\\source\\\\repos\\\\rrmistry\\\\kaggle\\\\NY_Taxi_Cab\\\\ML_Model', '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_log_step_count_steps': 100, '_task_type': 'worker', '_task_id': 0, '_device_fn': None, '_save_checkpoints_secs': 30, '_master': '', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 30.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 92948.58, step = 0\n",
      "INFO:tensorflow:global_step/sec: 8.31524\n",
      "INFO:tensorflow:loss = 71596.55, step = 100 (12.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.8312\n",
      "INFO:tensorflow:loss = 83864.195, step = 200 (11.322 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 238 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-16-22:30:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt-238\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-16-22:30:47\n",
      "INFO:tensorflow:Saving dict for global step 238: average_loss = 80.851, global_step = 238, label/mean = 11.595333, loss = 41395.71, prediction/mean = 10.881728, rmse = 8.991718\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 238: C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt-238\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'passengers': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'hourofday': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int32>, 'pickuplon': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dayofweek': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'passengers': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'hourofday': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int32>, 'pickuplon': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dayofweek': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt-238\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\export\\exporter\\temp-b'1537137047'\\saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 4.49804\n",
      "INFO:tensorflow:loss = 83790.71, step = 300 (22.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.78722\n",
      "INFO:tensorflow:loss = 84018.2, step = 400 (11.381 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 407 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.54178\n",
      "INFO:tensorflow:loss = 81172.74, step = 500 (11.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.82017\n",
      "INFO:tensorflow:loss = 62990.047, step = 600 (11.339 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 669 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.57295\n",
      "INFO:tensorflow:loss = 99333.945, step = 700 (11.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.76246\n",
      "INFO:tensorflow:loss = 66232.93, step = 800 (11.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.76578\n",
      "INFO:tensorflow:loss = 67948.84, step = 900 (11.409 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 930 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.49089\n",
      "INFO:tensorflow:loss = 69177.77, step = 1000 (11.777 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.80232\n",
      "INFO:tensorflow:loss = 77007.89, step = 1100 (11.360 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1191 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.55499\n",
      "INFO:tensorflow:loss = 101086.02, step = 1200 (11.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.7433\n",
      "INFO:tensorflow:loss = 82267.2, step = 1300 (11.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.76261\n",
      "INFO:tensorflow:loss = 72853.5, step = 1400 (11.412 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1451 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.5637\n",
      "INFO:tensorflow:loss = 93746.58, step = 1500 (11.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.77322\n",
      "INFO:tensorflow:loss = 134269.23, step = 1600 (11.399 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 8.75234\n",
      "INFO:tensorflow:loss = 137953.78, step = 1700 (11.426 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1712 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.5419\n",
      "INFO:tensorflow:loss = 77226.945, step = 1800 (11.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.7404\n",
      "INFO:tensorflow:loss = 73024.89, step = 1900 (11.441 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1973 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.52903\n",
      "INFO:tensorflow:loss = 87308.22, step = 2000 (11.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.76962\n",
      "INFO:tensorflow:loss = 88722.17, step = 2100 (11.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.76545\n",
      "INFO:tensorflow:loss = 89215.87, step = 2200 (11.408 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2233 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.39056\n",
      "INFO:tensorflow:loss = 102939.17, step = 2300 (11.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.68571\n",
      "INFO:tensorflow:loss = 90314.98, step = 2400 (11.513 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2491 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.49179\n",
      "INFO:tensorflow:loss = 88996.63, step = 2500 (11.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.73046\n",
      "INFO:tensorflow:loss = 108149.21, step = 2600 (11.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.76649\n",
      "INFO:tensorflow:loss = 78119.625, step = 2700 (11.407 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2751 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.32906\n",
      "INFO:tensorflow:loss = 67872.2, step = 2800 (12.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.3731\n",
      "INFO:tensorflow:loss = 91736.22, step = 2900 (11.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.67347\n",
      "INFO:tensorflow:loss = 71884.15, step = 3000 (11.529 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3004 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.35335\n",
      "INFO:tensorflow:loss = 98915.17, step = 3100 (11.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.61401\n",
      "INFO:tensorflow:loss = 79600.55, step = 3200 (11.609 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3260 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.42076\n",
      "INFO:tensorflow:loss = 78380.3, step = 3300 (11.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.92006\n",
      "INFO:tensorflow:loss = 106748.92, step = 3400 (12.627 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3501 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 7.90264\n",
      "INFO:tensorflow:loss = 76051.79, step = 3500 (12.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.28638\n",
      "INFO:tensorflow:loss = 74673.9, step = 3600 (12.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.83142\n",
      "INFO:tensorflow:loss = 70415.48, step = 3700 (12.769 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3739 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 7.59474\n",
      "INFO:tensorflow:loss = 85431.22, step = 3800 (13.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.38153\n",
      "INFO:tensorflow:loss = 73275.31, step = 3900 (11.931 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3983 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.07925\n",
      "INFO:tensorflow:loss = 81881.34, step = 4000 (12.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.36927\n",
      "INFO:tensorflow:loss = 122814.58, step = 4100 (11.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.36053\n",
      "INFO:tensorflow:loss = 106699.4, step = 4200 (11.961 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4231 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.05791\n",
      "INFO:tensorflow:loss = 60462.113, step = 4300 (12.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.37872\n",
      "INFO:tensorflow:loss = 55327.95, step = 4400 (11.935 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4479 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.11535\n",
      "INFO:tensorflow:loss = 74135.27, step = 4500 (12.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.39419\n",
      "INFO:tensorflow:loss = 67381.945, step = 4600 (11.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.38574\n",
      "INFO:tensorflow:loss = 84226.875, step = 4700 (11.925 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4728 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.16727\n",
      "INFO:tensorflow:loss = 61864.67, step = 4800 (12.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.36458\n",
      "INFO:tensorflow:loss = 104383.42, step = 4900 (11.954 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4977 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1278, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1263, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Failed to create a NewWriteableFile: C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt-5000.index : Access is denied.\r\n",
      "; Input/output error\n",
      "\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-13-d5d25f485900>\", line 28, in <module>\n",
      "    train_and_evaluate(arguments)\n",
      "  File \"<ipython-input-12-bb6e3c27f7fc>\", line 22, in train_and_evaluate\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 451, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 590, in run\n",
      "    return self.run_local()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 691, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 376, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1145, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1173, in _train_model_default\n",
      "    saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1451, in _train_with_estimator_spec\n",
      "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 695, in __exit__\n",
      "    self._close_internal(exception_type)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 727, in _close_internal\n",
      "    h.end(self._coordinated_creator.tf_sess)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\", line 470, in end\n",
      "    self._save(session, last_step)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\", line 481, in _save\n",
      "    self._get_saver().save(session, self._save_path, global_step=step)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1620, in save\n",
      "    {self.saver_def.filename_tensor_name: checkpoint_file})\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 877, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1100, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1272, in _do_run\n",
      "    run_metadata)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1291, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Failed to create a NewWriteableFile: C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt-5000.index : Access is denied.\r\n",
      "; Input/output error\n",
      "\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]\n",
      "\n",
      "Caused by op 'save/MergeV2Checkpoints', defined at:\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 497, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-d5d25f485900>\", line 28, in <module>\n",
      "    train_and_evaluate(arguments)\n",
      "  File \"<ipython-input-12-bb6e3c27f7fc>\", line 22, in train_and_evaluate\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 451, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 590, in run\n",
      "    return self.run_local()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 691, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 376, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1145, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1173, in _train_model_default\n",
      "    saving_listeners)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1448, in _train_with_estimator_spec\n",
      "    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 421, in MonitoredTrainingSession\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 832, in __init__\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 555, in __init__\n",
      "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1018, in __init__\n",
      "    _WrappedSession.__init__(self, self._create_session())\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1023, in _create_session\n",
      "    return self._sess_creator.create_session()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 712, in create_session\n",
      "    self.tf_sess = self._session_creator.create_session()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 474, in create_session\n",
      "    self._scaffold.finalize()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 214, in finalize\n",
      "    self._saver.build()\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1293, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1330, in _build\n",
      "    build_save=build_save, build_restore=build_restore)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 769, in _build_internal\n",
      "    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 360, in _AddShardedSaveOps\n",
      "    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 342, in _AddShardedSaveOpsForV2\n",
      "    sharded_prefixes, checkpoint_prefix, delete_old_dirs=True)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 495, in merge_v2_checkpoints\n",
      "    delete_old_dirs=delete_old_dirs, name=name)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"c:\\users\\mistr\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "UnknownError (see above for traceback): Failed to create a NewWriteableFile: C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model\\model.ckpt-5000.index : Access is denied.\r\n",
      "; Input/output error\n",
      "\t [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](save/MergeV2Checkpoints/checkpoint_prefixes, _arg_save/Const_0_0)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OUTPUTDIR = os.path.join(BASE_PATH, r'../ML_Model/')\n",
    "OUTPUTDIR = r'C:\\Users\\mistr\\source\\repos\\rrmistry\\kaggle\\NY_Taxi_Cab\\ML_Model'\n",
    "\n",
    "if os.path.exists(OUTPUTDIR):\n",
    "    shutil.rmtree(OUTPUTDIR,ignore_errors=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    arguments = {\n",
    "        \"output_dir\": OUTPUTDIR,\n",
    "        \"train_data_paths\": os.path.join(BASE_DATA_PATH, r'train_split\\train-*.csv'),\n",
    "        \"eval_data_paths\": os.path.join(BASE_DATA_PATH, r'train_split\\valid-*.csv'),\n",
    "        \"train_batch_size\": 1024,\n",
    "        \"eval_batch_size\": 512,\n",
    "        \"train_steps\": 5000,\n",
    "        \"eval_steps\": 10,\n",
    "        \"nbuckets\": 20,\n",
    "        \"hidden_units\": \"128 32 4\",\n",
    "        \"eval_delay_secs\": 10,\n",
    "        \"min_eval_frequency\": 1,\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "    \n",
    "    # Run the training job:\n",
    "    try:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        train_and_evaluate(arguments)\n",
    "        # print_datasets(arguments, sess)\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
