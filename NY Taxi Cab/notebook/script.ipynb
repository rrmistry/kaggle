{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b636e22977c7364a057d29195216501485c041a7"
   },
   "source": [
    "Rohit's First Kernal - NYC Taxi Fare Prediction\n",
    "===========\n",
    "This is the first kernal for submission for Google Cloud Playground [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction)\n",
    "\n",
    "Strategy\n",
    "--------------------\n",
    "1. Filter out outliers\n",
    "    1. Remove data outside NYC\n",
    "    2. Remove data where fare is unresonable (too high / too low)\n",
    "2. Use Linear Regression ML Model On Clean Data\n",
    "3. Use Linear Fit On Unclean Data\n",
    "\n",
    "Using NYC Open Data\n",
    "-------------------\n",
    "NYC Open Data is stored in Google Big Query open datasets. To access this data in your notebook, check out kernal [How to Query the NYC Open Data\n",
    "](https://www.kaggle.com/paultimothymooney/how-to-query-the-nyc-open-data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to plot 3d scatter plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import math\n",
    "\n",
    "# to print out current time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import traceback\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e78d5459cdfa29acfe87fc6282a41cc90bae2128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset -------------  2018-09-11 20:40:31.074105\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "print('Started reading dataset ------------- ', datetime.datetime.now())\n",
    "\n",
    "# Try to load the data. This may be an intensive process\n",
    "df_train = pd.read_csv(r'M:\\kaggle\\NY Taxi Cab\\input\\train.csv', parse_dates=[\"pickup_datetime\"]);\n",
    "\n",
    "print('Finished reading dataset ------------- ', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['key',\n",
    "               'fare_amount',\n",
    "               'pickup_datetime',\n",
    "               'pickup_longitude',\n",
    "               'pickup_latitude',\n",
    "               'dropoff_longitude',\n",
    "               'dropoff_latitude',\n",
    "               'passenger_count']\n",
    "\n",
    "LABEL_COLUMN = 'fare_amount' # 'pickup_datetime' #\n",
    "\n",
    "DEFAULTS = [['NoKey'],\n",
    "            [0.0],\n",
    "            ['BadDate'],\n",
    "            [-74.0],\n",
    "            [40.0],\n",
    "            [-74.0],\n",
    "            [40.7],\n",
    "            [1.0]]\n",
    "\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filenames, mode, batch_size = BATCH_SIZE):\n",
    "        \n",
    "    def _input_fn():\n",
    "        \n",
    "        def parse_dataset(filename, header_lines = 1):\n",
    "            return tf.data.TextLineDataset(filenames=filename).skip(header_lines) \n",
    "        \n",
    "        def parse_batch(value_column):\n",
    "            if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "                columns = tf.decode_csv(value_column, record_defaults = DEFAULTS[:1] + DEFAULTS[1:])\n",
    "                features = dict(zip(CSV_COLUMNS[:1] + CSV_COLUMNS[1:], columns))\n",
    "                label = DEFAULTS[1]\n",
    "            else:\n",
    "                columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
    "                features = dict(zip(CSV_COLUMNS, columns))\n",
    "                label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        filenames_dataset = tf.data.Dataset.list_files(filenames)\n",
    "        \n",
    "        # Read lines from text files\n",
    "        dataset = filenames_dataset.flat_map(parse_dataset)\n",
    "        \n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(parse_batch)\n",
    "        \n",
    "        # Note:\n",
    "        # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "        # use tf.data.Dataset.map            to apply one to one    transformations (here: text line -> feature list)\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                num_epochs = None # loop indefinitely\n",
    "                dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "                num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        \n",
    "        # Skip header row\n",
    "        return dataset.skip(1).make_one_shot_iterator().get_next()\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    return read_dataset('../input/train/train-*.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid():\n",
    "    return read_dataset('../input/train/test-*.csv', mode = tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "def get_test():\n",
    "    return read_dataset('../input/test.csv', mode = tf.estimator.ModeKeys.PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('pickup_longitude'),\n",
    "    tf.feature_column.numeric_column('pickup_latitude'),\n",
    "    tf.feature_column.numeric_column('dropoff_longitude'),\n",
    "    tf.feature_column.numeric_column('dropoff_latitude'),\n",
    "    tf.feature_column.numeric_column('passenger_count'),\n",
    "]\n",
    "\n",
    "def add_more_features(feats):\n",
    "    # Nothing to add (yet!)\n",
    "    return feats\n",
    "\n",
    "feature_cols = add_more_features(INPUT_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rmse(model, name, input_fn):\n",
    "    metrics = model.evaluate(input_fn = input_fn, steps = None)\n",
    "    print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = '../taxi_trained'\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "        \n",
    "        model = tf.estimator.LinearRegressor(feature_columns = feature_cols, model_dir = OUTDIR)\n",
    "        \n",
    "        print('Begin Training ---------------- ', datetime.datetime.now())\n",
    "        model.train(input_fn = get_train(), steps = 1000)\n",
    "        \n",
    "        print('Begin Testing ---------------- ', datetime.datetime.now())        \n",
    "        print_rmse(model, 'validation', get_valid())\n",
    "        \n",
    "        print('Finished Testing ---------------- ', datetime.datetime.now())   \n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
